# -*- coding: utf-8 -*-
"""dla2part2 (5).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kSvxv36dJos1vGgZZMcjf7uHqnbnKA_i
"""

import wandb
import argparse
import torch
import torchvision
import os
import shutil
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets
from torch.optim import Adam

#checking for device
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)


parser = argparse.ArgumentParser(description="Train VGG16 with WandB sweep support.")

# Development/debug options
parser.add_argument('--dev', action='store_true', help='Run in development mode (fewer batches).')
parser.add_argument('--sweep-count', type=int, default=10, help='Number of sweep runs to execute.')

# Data paths
default_data_path = '/kaggle/input/inaturalist12k/inaturalist_12K/train'
default_test_path = '/kaggle/input/inaturalist12k/inaturalist_12K/val'

parser.add_argument('--data-path', type=str, default=default_data_path,
                    help=f'Training data directory (default: {default_data_path})')
parser.add_argument('--test-path', type=str, default=default_test_path,
                    help=f'Validation/test data directory (default: {default_test_path})')

# WandB credentials
parser.add_argument('--wandb-key', type=str, default='3e2fb4c6e149ab3e746227ccd153039708a6d55a',
                    help='WandB API key')
parser.add_argument('--wandb-project', type=str, default='Assignment2',
                    help='WandB project name')
parser.add_argument('--wandb-entity', type=str, default='cs23m004-indian-institute-of-technology-madras',
                    help='WandB entity/team name')

args = parser.parse_args()

# Set global flags/paths using parsed arguments
DEV = args.dev
PATH_TRAIN_VAL = args.data_path
PATH_TEST = args.test_path
PROJECT_NAME = args.wandb_project
ENTITY = args.wandb_entity

# Login to WandB
wandb.login(key=args.wandb_key)

# Perform hyperparameter tuning with Weights & Biases Sweeps
config = {
    'method': 'bayes',
    'name': 'sweep1_partB2',
    'metric': {'name': 'valid_accuracy', 'goal': 'maximize'},
    'parameters': {
        'batch_size': {'values': [32, 64]},
        'learning_rate': {'values': [0.001, 0.0001]},
        'epochs': {'values': [2, 3, 4]},
        'num_added_layers': {'values': [1,2,3]},
        'num_neurons': {'values': [512, 1024, 2048, 4096]},
        'optimizer_name': {'values': ['Adam', 'SGD']}
    }
}

def get_data(data_path=PATH_TRAIN_VAL, batch_size=32, augmentation=False):
    '''
    Loads and splits the dataset from the specified directory into training and validation sets.

    By default, it performs an 80-20 random split, with 20% of the data used for validation.
    If the `augmentation` flag is set to True, additional data augmentation techniques
    are applied to the training dataset.

    The function returns two DataLoader objects: one for the training set and one for the validation set.

    Parameters:
        data_path (str): Path to the directory containing the dataset.
        augmentation (bool): Whether to apply data augmentation to the training data.

    Returns:
        train_loader, val_loader: DataLoader instances for training and validation.
'''

   # Set up preprocessing and augmentation pipelines for training and validation
    data_transforms = {
        "val": transforms.Compose([
            transforms.Resize((256, 256)),                        # Apply same resizing for validation
            transforms.ToTensor(),                                # Convert to tensor format
            transforms.Normalize(                                 # Apply normalization
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ]),
        "train": transforms.Compose([
            transforms.Resize((256, 256)),                        # Resize all training images
            transforms.RandomHorizontalFlip(),                    # Randomly flip images horizontally
            transforms.ToTensor(),                                # Convert images to PyTorch tensors
            transforms.Normalize(                                 # Normalize using ImageNet stats
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

    }

    totalsize = 0
    aug = augmentation
    if aug :
        print("applying Augmentations")
    if aug:
        # Apply augmentations
        extra_transforms = [
            transforms.RandomVerticalFlip(),                      # Random vertical flip
            transforms.RandomRotation(20),                        # Rotate images randomly up to 20 degrees
            transforms.ColorJitter(
                brightness=0.1, contrast=0.1,
                saturation=0.1, hue=0.1                           # Slight color distortion
            )
        ]
        data_transforms['train'].transforms.insert(2, extra_transforms[0])  # Vertical flip
        data_transforms['train'].transforms.insert(3, extra_transforms[1])  # Rotation
        data_transforms['train'].transforms.insert(4, extra_transforms[2])  # Color jitter


    # Load the datasets using ImageFolder
    image_datasets = {
        "train": datasets.ImageFolder(data_path, data_transforms["train"]),
        "val": datasets.ImageFolder(data_path, data_transforms["val"])
    }

    # Perform an 80-20 split on the training dataset
    splitratio = 0.8
    ttlsize = len(image_datasets["train"])
    train_size = int(splitratio * ttlsize)
    val_size = len(image_datasets["train"]) - train_size
    print("")
    train_dataset, val_dataset = random_split(image_datasets["train"], [train_size, val_size])

    # Creating dataloaders for train and validation sets
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    print("")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    print("Dataloaders Created")

    return train_loader, val_loader

def get_vgg16_model(output_feature=10, num_added_layers=1, num_neurons=512):
    # Load the pre-trained VGG16 model with ImageNet weights
    model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)

    # Freeze all layers to prevent updates during backpropagation
    list(map(lambda p: setattr(p, 'requires_grad', False), model.parameters()))


    # Get input size of the original final fully connected layer
    in_features = model.classifier[6].in_features

    # Retrieve all layers except the final one
    layers = list(nn.Sequential(*model.classifier).children())[:-1]


    # Add custom fully connected layers
    for layer_idx in range(num_added_layers):
        linear_layer = nn.Linear(in_features, num_neurons)
        activation = nn.ReLU(inplace=True)
        dropout_layer = nn.Dropout()

        layers.extend([linear_layer, activation, dropout_layer])

        # Prepare for the next added layer
        in_features = num_neurons


    # Final classification layer with output_feature classes
    layers.append(nn.Linear(num_neurons, output_feature))
    print("layers added")
    # Set the modified classifier back to the model
    model.classifier = nn.Sequential(*layers)

    return model

# Training routine for the model
def train(config=None):
    # Start a new Weights & Biases run
    print("starting training process : ")
    with wandb.init(project=PROJECT_NAME, entity=ENTITY, config=config) as run:
        # Extract configuration settings
        cfg = wandb.config
        print("Wandb initialised")
        # Prepare the training and validation data
        train_loader, val_loader = get_data(batch_size=cfg.batch_size)

        # Build the model using the specified config
        model = get_vgg16_model(output_feature=10,num_added_layers=cfg.num_added_layers,num_neurons=cfg.num_neurons).to(device)

        # Set up the optimizer and the loss function
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)

        # Begin training for the specified number of epochs
        for epoch in range(cfg.epochs):
            print(f"Epoch {epoch + 1}")

            model.train()
            epoch_loss = 0.0
            correct_preds = 0
            total_samples = 0
            batches_processed = 0

            # Iterate through training data
            for idx, (inputs, labels) in enumerate(train_loader):
                if idx % 20 == 0:
                    print(f"Training batch {idx}")

                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                epoch_loss = epoch_loss+loss.item()
                _, predictions = torch.max(outputs, 1)
                total_samples = total_samples+labels.size(0)
                predi = (predictions == labels).sum().item()
                correct_preds = correct_preds + predi

                wandb.log({"batch_loss": loss.item()})
                batches_processed = batches_processed + 1

                if DEV:
                    if idx > 5:
                        print(f"Exiting early in DEV mode : batch {idx}")
                        break


            train_loss = 0.0
            train_loss = epoch_loss / batches_processed
            train_accuracy = 0.0
            train_accuracy = correct_preds / total_samples

            # Switch to evaluation mode for validation
            model.eval()
            val_loss = 0.0
            correct_preds = 0
            total_samples = 0
            batches_processed = 0

            with torch.no_grad():
                for idx, (inputs, labels) in enumerate(val_loader):
                    inputs, labels = inputs.to(device), labels.to(device)
                    temp = 0
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)

                    val_loss = val_losss + loss.item()
                    _, predictions = torch.max(outputs, 1)
                    total_samples = total_samples + labels.size(0)
                    predi = (predictions == labels).sum().item()
                    correct_preds = correct_preds + predi

                    batches_processed = batches_processed+1

                    if DEV:
                        if idx > 10:
                            print("DEV mode: exiting now")
                            break

            val_loss /= batches_processed
            val_accuracy = 0.0
            val_accuracy = correct_preds / total_samples

            # Log performance metrics for the epoch
            wandb.log({"epoch": epoch + 1,"train_loss": train_loss,"train_accuracy": train_accuracy,"valid_loss": val_loss,"valid_accuracy": val_accuracy})


# Run the W&B sweep
sweep_id = wandb.sweep(config, project=PROJECT_NAME, entity=ENTITY)
wandb.agent(sweep_id, function=train, count=args.sweep_count)

