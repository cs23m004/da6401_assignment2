{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0c2cf47e9fe84fbe8280ce9a0e995c77":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_e5235869524342de83221ccec101b0fb","IPY_MODEL_45eda9efd5694d00ace2caf9b68140c3"],"layout":"IPY_MODEL_909f608b452b43aa8bdb42a610ec373a"}},"e5235869524342de83221ccec101b0fb":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5bd636500754870ab993eb9bb53a672","placeholder":"â€‹","style":"IPY_MODEL_e169879961e24f279d818c89a84ec96a","value":"0.011 MB of 0.011 MB uploaded\r"}},"45eda9efd5694d00ace2caf9b68140c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_345c14171e094bf7bee0bdfb6f0acff5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c4df9181811a45d89faeaa0cee86e21c","value":1}},"909f608b452b43aa8bdb42a610ec373a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5bd636500754870ab993eb9bb53a672":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e169879961e24f279d818c89a84ec96a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"345c14171e094bf7bee0bdfb6f0acff5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4df9181811a45d89faeaa0cee86e21c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11450916,"sourceType":"datasetVersion","datasetId":7174477}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nimport torch\nimport torchvision\nimport os\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets\nfrom torch.optim import Adam","metadata":{"id":"8WOvqnZn2y5e","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T11:32:13.087431Z","iopub.execute_input":"2025-04-19T11:32:13.087647Z","iopub.status.idle":"2025-04-19T11:32:22.814004Z","shell.execute_reply.started":"2025-04-19T11:32:13.087619Z","shell.execute_reply":"2025-04-19T11:32:22.813390Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#checking for device\ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"id":"poBKVE1TzzX8","outputId":"5705b8da-b02c-474f-8f2c-055fab69c837","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T11:32:22.814569Z","iopub.execute_input":"2025-04-19T11:32:22.814914Z","iopub.status.idle":"2025-04-19T11:32:22.876156Z","shell.execute_reply.started":"2025-04-19T11:32:22.814895Z","shell.execute_reply":"2025-04-19T11:32:22.875480Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"wandb.login(key='3e2fb4c6e149ab3e746227ccd153039708a6d55a')\nPROJECT_NAME = 'Assignment2'\nENTITY = 'cs23m004-indian-institute-of-technology-madras'","metadata":{"id":"SAOqbFJFqFOP","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T11:32:22.878761Z","iopub.execute_input":"2025-04-19T11:32:22.879360Z","iopub.status.idle":"2025-04-19T11:32:28.704591Z","shell.execute_reply.started":"2025-04-19T11:32:22.879338Z","shell.execute_reply":"2025-04-19T11:32:28.704024Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m004\u001b[0m (\u001b[33mcs23m004-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Define the data directory\nDATA_DIR = '/kaggle/input/inaturalist12k/inaturalist_12K'\nPATH_TRAIN_VAL = os.path.join(DATA_DIR, 'train')\nPATH_TEST = os.path.join(DATA_DIR, 'test')","metadata":{"id":"ee9e2MZEMO6V","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T11:32:28.705311Z","iopub.execute_input":"2025-04-19T11:32:28.705748Z","iopub.status.idle":"2025-04-19T11:32:28.709626Z","shell.execute_reply.started":"2025-04-19T11:32:28.705713Z","shell.execute_reply":"2025-04-19T11:32:28.708917Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"config = {\n    \"method\": \"bayes\",\n    \"name\": \"fix\",\n    \"project\": \"Assignment2\",\n    \"entity\": \"cs23m004-indian-institute-of-technology-madras\",\n    \"metric\": {\n        \"name\": \"valid_accuracy\",\n        \"goal\": \"maximize\"\n    },\n    \"parameters\": {\n        \"num_filters\": {\n            \"values\": [\n                [32, 64, 128, 256, 512]\n            ]\n        },\n        \"conv_filter_size\": {\n            \"values\": [\n                [7, 7, 3, 3, 3],\n                [3, 3, 3, 3, 3],\n                [3, 3, 3, 7, 7]\n            ]\n        },\n        \"batch_size\": {\n            \"values\": [16, 32, 64]\n        },\n        \"conv_activation\": {\n            \"values\": [\n                [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\"]\n            ]\n        },\n        \"dense_activation\": {\n            \"values\": [\"relu\", \"sigmoid\"]\n        },\n        \"pool_filter_size\": {\n            \"values\": [\n                [2, 2, 2, 2, 2]\n            ]\n        },\n        \"dense_layer\": {\n            \"values\": [32, 64, 128, 512]\n        },\n        \"dropout\": {\n            \"values\": [0.0, 0.4, 0.5, 0.6]\n        },\n        \"learning_rate\": {\n            \"values\": [0.001, 0.0001]\n        },\n        \"num_epochs\": {\n            \"values\": [10, 15, 20, 25]\n        },\n        \"data_augmentation\": {\n            \"values\": [True]\n        },\n        \"batch_norm\": {\n            \"values\": [True, False]\n        }\n    }\n}\n","metadata":{"id":"fU9LPNrbbend","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T11:32:28.710349Z","iopub.execute_input":"2025-04-19T11:32:28.710561Z","iopub.status.idle":"2025-04-19T11:32:28.912558Z","shell.execute_reply.started":"2025-04-19T11:32:28.710544Z","shell.execute_reply":"2025-04-19T11:32:28.911809Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def get_data(data_path=PATH_TRAIN_VAL, batch_size=32, augmentation=False):\n    '''\n    Loads and splits the dataset from the specified directory into training and validation sets.\n\n    By default, it performs an 80-20 random split, with 20% of the data used for validation.\n    If the `augmentation` flag is set to True, additional data augmentation techniques\n    are applied to the training dataset.\n\n    The function returns two DataLoader objects: one for the training set and one for the validation set.\n\n    Parameters:\n        data_path (str): Path to the directory containing the dataset.\n        augmentation (bool): Whether to apply data augmentation to the training data.\n\n    Returns:\n        train_loader, val_loader: DataLoader instances for training and validation.\n'''\n\n   # Set up preprocessing and augmentation pipelines for training and validation\n    data_transforms = {\n        \"val\": transforms.Compose([\n            transforms.Resize((256, 256)),                        # Apply same resizing for validation\n            transforms.ToTensor(),                                # Convert to tensor format\n            transforms.Normalize(                                 # Apply normalization\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ]),\n        \"train\": transforms.Compose([\n            transforms.Resize((256, 256)),                        # Resize all training images\n            transforms.RandomHorizontalFlip(),                    # Randomly flip images horizontally\n            transforms.ToTensor(),                                # Convert images to PyTorch tensors\n            transforms.Normalize(                                 # Normalize using ImageNet stats\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n    }\n\n    totalsize = 0\n    aug = augmentation\n    if aug :\n        print(\"applying Augmentations\")\n    if aug:\n        # Apply augmentations\n        extra_transforms = [\n            transforms.RandomVerticalFlip(),                      # Random vertical flip\n            transforms.RandomRotation(20),                        # Rotate images randomly up to 20 degrees\n            transforms.ColorJitter(                               \n                brightness=0.1, contrast=0.1,                     \n                saturation=0.1, hue=0.1                           # Slight color distortion\n            )\n        ]\n        data_transforms['train'].transforms.insert(2, extra_transforms[0])  # Vertical flip\n        data_transforms['train'].transforms.insert(3, extra_transforms[1])  # Rotation\n        data_transforms['train'].transforms.insert(4, extra_transforms[2])  # Color jitter\n\n    \n    # Load the datasets using ImageFolder\n    image_datasets = {\n        \"train\": datasets.ImageFolder(data_path, data_transforms[\"train\"]),\n        \"val\": datasets.ImageFolder(data_path, data_transforms[\"val\"])\n    }\n    \n    # Perform an 80-20 split on the training dataset\n    splitratio = 0.8\n    ttlsize = len(image_datasets[\"train\"])\n    train_size = int(splitratio * ttlsize)\n    val_size = len(image_datasets[\"train\"]) - train_size\n    print(\"\")\n    train_dataset, val_dataset = random_split(image_datasets[\"train\"], [train_size, val_size])\n    \n    # Creating dataloaders for train and validation sets\n    print(\"\")\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    \n    return train_loader, val_loader","metadata":{"id":"XWzQpxPrzqR_","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T11:32:28.913301Z","iopub.execute_input":"2025-04-19T11:32:28.913488Z","iopub.status.idle":"2025-04-19T11:32:28.928859Z","shell.execute_reply.started":"2025-04-19T11:32:28.913473Z","shell.execute_reply":"2025-04-19T11:32:28.928163Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class CNN(nn.Module):\n    \n    def __init__(self, img_shape, conv_activation, dense_activation, num_filters, conv_filter_size, pool_filter_size, batch_norm, dense_layer, dropout):\n        \n        super(CNN, self).__init__()\n        print(\"\")\n        # Save configuration parameters\n        # Store initialization parameters\n        self.batch_norm = batch_norm\n        self.img_shape = img_shape\n        self.conv_activation = conv_activation\n        print(\"\")\n        self.num_filters = num_filters\n        self.pool_filter_size = pool_filter_size\n        self.dense_activation = dense_activation\n        print(\"\")\n        self.conv_filter_size = conv_filter_size\n        self.dropout = dropout\n        self.dense_layer = dense_layer\n        \n        # Initialize the convolutional layer sequence\n        self.conv_layers = nn.Sequential()\n        print(\"\")\n        # Extract the number of channels from the input shape\n        input_channels = img_shape[0]\n\n\n       # Loop through each convolutional layer configuration\n        for i in range(5):\n            # Add convolutional layer\n            self.conv_layers.add_module(f'conv{i+1}',nn.Conv2d(in_channels=input_channels,out_channels=num_filters[i],kernel_size=conv_filter_size[i]))\n        \n            # Optionally include batch normalization\n            if batch_norm: print(\"applying batch normalisation\")\n            if batch_norm:\n                self.conv_layers.add_module(f'bn{i+1}', nn.BatchNorm2d(num_filters[i]))\n        \n            # Select activation function based on config\n            if conv_activation[i] == 'sigmoid':\n                 activation_fn = nn.Sigmoid()\n            else :\n                 activation_fn = nn.ReLU()\n            \n            self.conv_layers.add_module(f'activation{i+1}', activation_fn)\n        \n            # Apply max pooling layer\n            self.conv_layers.add_module(f'pool{i+1}',nn.MaxPool2d(kernel_size=pool_filter_size[i],stride=2))\n        \n            # Update input channel count for the next conv layer\n            input_channels = num_filters[i]\n\n        # Compute the flattened size after conv layers by forwarding a dummy input\n        dummy_input = torch.randn(1, *img_shape)\n        with torch.no_grad():\n            dummy_output = self.conv_layers(dummy_input)\n        self.flatten_dim = dummy_output.view(1, -1).size(1)\n\n        # Fully connected layers\n        self.fc_layers = nn.Sequential(nn.Linear(self.flatten_dim, dense_layer),nn.Sigmoid() if dense_activation == 'sigmoid' else nn.ReLU(),nn.Dropout(dropout),\n            nn.Linear(dense_layer, 10)  # 10 output classes\n        )\n\n    \n    \n  \n    \n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(x.size(0), -1)  # Flatten the output\n        x = self.fc_layers(x)\n        return x","metadata":{"id":"XWzQpxPrzqR_","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T11:32:28.929545Z","iopub.execute_input":"2025-04-19T11:32:28.929777Z","iopub.status.idle":"2025-04-19T11:32:28.949495Z","shell.execute_reply.started":"2025-04-19T11:32:28.929752Z","shell.execute_reply":"2025-04-19T11:32:28.948684Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train():\n    # Initialize a new Weights & Biases run\n    with wandb.init(project=PROJECT_NAME, entity=ENTITY):\n        print(\"Wandb initiates\")\n        config = wandb.config  # Access hyperparameter config from wandb\n\n        # Load training and validation data using specified batch size and augmentation flag\n        train_loader, val_loader = get_data(augmentation=config.data_augmentation,batch_size=config.batch_size)\n        # Create a unique name for the run based on configuration settings\n        run_name = (\n            f\"batch_size={config.batch_size}_\"\n            f\"batch_norm={config.batch_norm}_\"\n            f\"num_filters={config.num_filters}_\"\n            f\"conv_activation={config.conv_activation}_\"\n            f\"dense_activation={config.dense_activation}_\"\n            f\"conv_filter_size={config.conv_filter_size}_\"\n            f\"pool_filter_size={config.pool_filter_size}_\"\n            f\"dense_layer={config.dense_layer}_\"\n            f\"dropout={config.dropout}_\"\n            f\"learning_rate={config.learning_rate}_\"\n            f\"num_epochs={config.num_epochs}_\"\n            f\"data_augmentation={config.data_augmentation}\"\n        )\n        wandb.run.name = run_name  # Assign the generated name to the run\n\n        # CNN model using the provided config\n        model = CNN(\n            conv_activation=config.conv_activation,\n            num_filters=config.num_filters,\n            dense_activation=config.dense_activation,\n            img_shape=(3, 256, 256),\n            conv_filter_size=config.conv_filter_size,\n            pool_filter_size=config.pool_filter_size,\n            batch_norm=config.batch_norm,\n            dense_layer=config.dense_layer,\n            dropout=config.dropout\n        ).to(device)\n        print(\"Model initialised\")\n\n        # Define loss function and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = Adam(model.parameters(), lr=config.learning_rate)\n\n        # Training process begins\n        for epoch in range(config.num_epochs):\n            print(f\"epoch : {epoch + 1}\")\n            model.train()\n            train_loss, correct_preds, total_samples = 0.0, 0, 0\n\n            for batch_idx, (inputs, labels) in enumerate(train_loader):\n                if batch_idx % 20 == 0:\n                    print(f\"batch {batch_idx}\")\n                    \n                # Move data to appropriate device (CPU or GPU)\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                # Forward pass and backpropagation\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                # Log batch loss to wandb\n                wandb.log({\"batch_loss\": loss.item()})\n\n                # Update training metrics\n                train_loss = train_loss+loss.item()\n                _, predictions = torch.max(outputs, 1)\n                total_samples = total_samples+labels.size(0)\n                correct_preds += (predictions == labels).sum().item()\n\n            # Calculate average training loss and accuracy\n            lenloader = len(train_loader)\n            avg_train_loss = train_loss / lenloader\n            train_accuracy = correct_preds / total_samples\n\n            # Validation phase\n            model.eval()\n            val_loss, val_correct, val_total = 0.0, 0, 0\n\n            with torch.no_grad():\n                for inputs, labels in val_loader:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    val_loss = val_loss + loss.item()\n                    _, predictions = torch.max(outputs, 1)\n                    val_total = val_total + labels.size(0)\n                    correctpred = (predictions == labels).sum().item()\n                    val_correct = val_correct + correctpred \n\n            # Compute validation metrics\n            val_len = len(val_loader)\n            avg_val_loss = val_loss / val_len\n            val_accuracy = val_correct / val_total\n\n            # Log metrics to wandb for this epoch\n            wandb.log({\n                \"epoch\": epoch + 1,\n                \"train_loss\": avg_train_loss,\n                \"train_accuracy\": train_accuracy,\n                \"valid_loss\": avg_val_loss,\n                \"valid_accuracy\": val_accuracy\n            })\n\n            # Print epoch summary\n            print(\n                f\"Epoch [{epoch + 1}/{config.num_epochs}] \"\n                f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n                f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\"\n            )\n","metadata":{"id":"XWzQpxPrzqR_","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T11:32:28.950327Z","iopub.execute_input":"2025-04-19T11:32:28.950622Z","iopub.status.idle":"2025-04-19T11:32:28.970358Z","shell.execute_reply.started":"2025-04-19T11:32:28.950598Z","shell.execute_reply":"2025-04-19T11:32:28.969759Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Initialize and run the sweep\nsweep_id = wandb.sweep(config)\nwandb.agent(sweep_id, function=train, count = 1)\n","metadata":{"id":"7g_Ox2GRHOzU","outputId":"c86b280a-5efc-405d-e2c9-b0eedd327ba6","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T11:32:28.972237Z","iopub.execute_input":"2025-04-19T11:32:28.972483Z","execution_failed":"2025-04-19T11:33:29.538Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: 1u7mw8kg\nSweep URL: https://wandb.ai/cs23m004-indian-institute-of-technology-madras/Assignment2/sweeps/1u7mw8kg\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bb1eq2kj with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: ['relu', 'relu', 'relu', 'relu', 'relu']\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filter_size: [3, 3, 3, 7, 7]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: relu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_layer: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 25\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: [32, 64, 128, 256, 512]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpool_filter_size: [2, 2, 2, 2, 2]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Assignment2' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring entity 'cs23m004-indian-institute-of-technology-madras' when running a sweep."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_113235-bb1eq2kj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m004-indian-institute-of-technology-madras/Assignment2/runs/bb1eq2kj' target=\"_blank\">smooth-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m004-indian-institute-of-technology-madras/Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23m004-indian-institute-of-technology-madras/Assignment2/sweeps/1u7mw8kg' target=\"_blank\">https://wandb.ai/cs23m004-indian-institute-of-technology-madras/Assignment2/sweeps/1u7mw8kg</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m004-indian-institute-of-technology-madras/Assignment2' target=\"_blank\">https://wandb.ai/cs23m004-indian-institute-of-technology-madras/Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23m004-indian-institute-of-technology-madras/Assignment2/sweeps/1u7mw8kg' target=\"_blank\">https://wandb.ai/cs23m004-indian-institute-of-technology-madras/Assignment2/sweeps/1u7mw8kg</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m004-indian-institute-of-technology-madras/Assignment2/runs/bb1eq2kj' target=\"_blank\">https://wandb.ai/cs23m004-indian-institute-of-technology-madras/Assignment2/runs/bb1eq2kj</a>"},"metadata":{}},{"name":"stdout","text":"Wandb initiates\napplying Augmentations\n\n\n\n\n\n\napplying batch normalisation\napplying batch normalisation\napplying batch normalisation\napplying batch normalisation\napplying batch normalisation\nModel initialised\nepoch : 1\nbatch 0\nbatch 20\nbatch 40\n","output_type":"stream"}],"execution_count":null}]}